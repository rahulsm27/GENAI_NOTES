{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "RLHF is an important tuning technique that has \n",
    "been critical to align an LLM's output with human preferences and values. \n",
    "\n",
    "\n",
    "While fine-tuning could be one way to do this, as \n",
    "you learn in this course, for many cases, RLHF \n",
    "can be more efficient.\n",
    "\n",
    "\n",
    "In this process, you start off with an LLM that's \n",
    "already been trained with instruction tuning, so \n",
    "it's already learned to follow instructions. \n",
    "You then gather a dataset that indicates a human label's \n",
    "preferences between multiple completions of the \n",
    "same prompt, and use this dataset as \n",
    "a reward signal, or to create a reward signal, to \n",
    "fine-tune an instruction an instruction tuned LLM. \n",
    "The result is a tuned large language model that \n",
    "generates completions or outputs that better aligns with the \n",
    "preferences of the human labelers\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
