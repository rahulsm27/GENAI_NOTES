{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "Re-ranking is a way to order results and score \n",
    "them according to their relevancy to a particular \n",
    "query. \n",
    "\n",
    "\n",
    "your re-ranking model scores each of the results conditioned on the \n",
    "query, and those with the highest score are the most \n",
    "relevant. \n",
    "Then you can just select the top ranking \n",
    "results as the most relevant to your particular query. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](image-7.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Cross encoder\n",
    "In contrast, a BERT \n",
    "cross-encoder takes both our query and our document and passes it \n",
    "through a classifier which outputs a score. \n",
    "And in this way, we can use our cross-encoder \n",
    "to score our retrieved results by \n",
    "passing our query and each retrieved document and scoring \n",
    "them using the cross-encoder. We can use the cross-encoder by passing \n",
    "in the original query and each one of the retrieved documents and \n",
    "using the resulting score as a relevancy or ranking \n",
    "score for our retrieved results. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](image-8.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](image-9.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
